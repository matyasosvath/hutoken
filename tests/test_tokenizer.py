from typing import Any, cast

import timeit
import pytest
import tiktoken
import transformers

import hutoken


sentence1 = "How can the net amount of entropy of the universe be massively decreased?"
sentence2 = "What I cannot create, I do not understand."
paragraph1 = (
    "Gorcsev Iv√°n, a Rangoon teherhaj√≥ matr√≥za m√©g huszonegy √©ves sem volt, mid≈ën elnyerte a fizikai Nobel-d√≠jat."
    "Ilyen nagy jelent≈ës√©g≈± tudom√°nyos jutalmat e po√©tikusan ifj√∫ korban megszerezni p√©ld√°tlan nagyszer≈± teljes√≠tm√©ny,"
    "m√©g akkor is, ha egyesek el≈ëtt tal√°n sz√©ps√©ghib√°nak t≈±nik majd, hogy Gorcsev Iv√°n a fizikai Nobel-d√≠jat "
    "a makao nev≈± k√°rtyaj√°t√©kon nyerte el, Noah Bertinus professzort√≥l, akinek ezt a kit√ºntet√©st Stockholmban,"
    "n√©h√°ny nappal el≈ëbb, a sv√©d kir√°ly ny√∫jtotta √°t, de v√©gre is a k√°k√°n csom√≥t keres≈ëk nem sz√°m√≠tanak; "
    "a l√©nyeg a f≈ë: hogy Gorcsev Iv√°n igenis huszonegy √©ves kor√°ban elnyerte a Nobel-d√≠jat."
)
paragraph2 = (
    "Hogyan hatott r√°tok, ath√©ni f√©rfiak, v√°dl√≥im besz√©de, nem tudom; √©n bizony magam is kis h√≠j√°n bel√©j√ºk feledkeztem,"
    "olyan meggy≈ëz≈ëen besz√©ltek. √Åmb√°r igazat √∫gysz√≥lv√°n semmit nem mondtak. Sok hazugs√°guk k√∂z√ºl legink√°bb egyet csod√°ltam:"
    "azt, amelyben kijelentett√©k, hogy √≥vakodnotok kell, nehogy r√°szedjelek, minthogy f√©lelmes sz√≥nok vagyok. Nem sz√©gyellt√©k,"
    "hogy nyomban tettel c√°folok r√°juk, minthogy egy√°ltal√°n nem mutatkozom f√©lelmes sz√≥noknak; ez volt, gondolom, a legnagyobb"
    "sz√©gyentelens√©g√ºk; hacsak nem azt nevezik ≈ëk f√©lelmes sz√≥noknak, aki igazat mond. Mert ha igen, akkor elismerem:"
    "sz√≥nok vagyok, b√°r nem olyan, amilyennek ≈ëk elk√©pzelik. Mert ≈ëk, mint mondom, alig mondtak valami igazat vagy √©ppen"
    "semmit sem; t≈ëlem viszont a tiszta igazs√°got fogj√°tok hallani. De bizony, Zeuszra, ath√©ni f√©rfiak, nem √©kes szav√∫,"
    "sz√≥lamokkal √©s cifra kifejez√©sekkel f√∂ld√≠sz√≠tett besz√©deket, mint amilyeneket amazok mondanak, hanem az √©ppen k√≠n√°lkoz√≥"
    "szavakb√≥l √°ll√≥, keresetlen besz√©det. Mert hiszem, hogy mind igaz, amit mondok, √©s senki k√∂z√ºletek t≈ëlem m√°st ne v√°rjon:"
    "mert hiszen nem is illen√©k, f√©rfiak, hogy ebben az √©letkorban √∫gy l√©pjek el√©tek, mint valami besz√©deket el≈ëre mint√°zgat√≥ ifjonc."
    "De √©ppen ez√©rt k√©rve k√©rlek, ath√©ni f√©rfiak: ha azt hallan√°tok, hogy ugyanolyan szavakkal v√©dekezem, mint amilyenekkel a"
    "piacon, a p√©nzv√°lt√≥k asztalain√°l - ahol sokatok hallgatott m√°r engem - vagy m√°sutt szoktam besz√©lni, ne csod√°lkozzatok,"
    "ne is zajongjatok miatta. Mert √≠gy √°ll a dolog: most √°llok el≈ësz√∂r t√∂rv√©nysz√©k el≈ëtt, t√∂bb mint hetvenesztend≈ës koromban,"
    "√©s sz√∂rnyen idegen sz√°momra az itt szok√°sos besz√©dm√≥d. √çgy h√°t, mint ahogy, ha t√∂rt√©netesen val√≥ban idegen voln√©k, megengedn√©tek,"
    "hogy azon a nyelven √©s azon a m√≥don sz√≥ljak, amelyen nevelkedtem, √©ppen √∫gy most is azzal a jogos, "
    "legal√°bbis nekem jogosnak tetsz≈ë k√©r√©ssel fordulok hozz√°tok, hogy adjatok enged√©lyt saj√°t besz√©dmodoromra - lehet, hogy "
    "ez √≠gy rosszabb lesz, de lehet, hogy jobb; √°m csak azt kell vizsg√°lnotok, arra kell ford√≠tan√≥tok figyelmeteket, "
    "vajon igazat mondok-e vagy nem: a b√≠r√≥nak ugyanis ez az er√©nye, a sz√≥nok√© pedig az igazmond√°s."
)

@pytest.fixture(autouse=False)
def setup():

    hutoken.initialize('./vocabs/gpt2-vocab.txt')

    tt_enc = tiktoken.get_encoding("gpt2")

    hf_enc = cast(Any, transformers).GPT2TokenizerFast.from_pretrained("gpt2")
    hf_enc.model_max_length = 1e30  # silence!

    yield tt_enc, hf_enc


def test_encode_raises_error():

    with pytest.raises(RuntimeError, match="Vocabulary is not initialized for encoding. Call 'initialize_encode' function first."):
        hutoken.encode("szia")


def test_decode_raises_error():

    with pytest.raises(RuntimeError, match="Vocabulary is not initialized for decoding. Call 'initialize_decode' function first."):
        hutoken.decode([1,2,3])


def test_encode_basic(setup):

    tt_enc, hf_enc = setup

    assert hutoken.encode(sentence1) == tt_enc.encode(sentence1)
    # assert hutoken.encode(sentence2) == tt_enc.encode(sentence2)
    # assert hutoken.encode(paragraph1) == tt_enc.encode(paragraph1)
    # assert hutoken.encode(paragraph2) == tt_enc.encode(paragraph2)


def test_decode_basic(setup):

    assert hutoken.decode(hutoken.encode(sentence1)) == sentence1
    # assert hutoken.decode(hutoken.encode(sentence2)) == sentence2
    # assert hutoken.decode(hutoken.encode(paragraph1)) == paragraph1
    # assert hutoken.decode(hutoken.encode(paragraph2)) == paragraph2


@pytest.mark.benchmark(disable_gc=True)
def test_encode_speed():

    number = 10_000
    execution_time = timeit.timeit(
        f'hutoken.encode("{sentence1}")',
        setup="import hutoken; hutoken.initialize('./vocabs/gpt2-vocab.txt')",
        number=number
    )
    print(f"Average execution time for {number} calls: {execution_time / number} seconds")

    assert execution_time / number < 1e-03, f"Average exectuion for function took too long: {execution_time / number}."


def test_decode_speed():

    number = 10_000
    execution_time = timeit.timeit(
        f"hutoken.decode(hutoken.encode('{sentence1}'))",
        setup="import hutoken; hutoken.initialize('./vocabs/gpt2-vocab.txt')",
        number=number
    )

    print(f"Average execution time for {number} calls: {execution_time / number} seconds")

    assert execution_time / number < 1e-03, f"Average exectuion for function took too long: {execution_time / number}."


def test_initialize_success():
    """Test successful initialization of the tokenizer."""
    hutoken.initialize('./vocabs/gpt2-vocab.txt')


def test_initialize_invalid_format():
    """Test that initialize raises ValueError for an invalid vocab file format."""
    with open('./vocabs/invalid-vocab.txt', 'w') as f:
        f.write("invalid_line_format\n")
    with pytest.raises(ValueError, match="Vocab file is empty or contains no valid entries."):
        hutoken.initialize('./vocabs/invalid-vocab.txt')
        
        
def test_encode():
    """Test the encode function of hutoken."""
    hutoken.initialize('./vocabs/gpt2-vocab.txt')

    text = "Hello, world!"
    encoded_tokens = hutoken.encode(text)

    assert isinstance(encoded_tokens, list), "Encoded result should be a list"
    assert all(isinstance(token, int) for token in encoded_tokens), "All elements in the encoded result should be integers"

    print(f"Encoded tokens: {encoded_tokens}")
    

def test_decode_invalid_tokens():
    """Test that decode raises ValueError for invalid tokens."""
    hutoken.initialize('./vocabs/gpt2-vocab.txt')

    invalid_tokens = [999999, -1, 50258]  # Out of bounds or invalid tokens
    with pytest.raises(ValueError, match="Element must be non-negative and less than vocab size."):
        hutoken.decode(invalid_tokens)


def test_decode_using_tiktoken_encode():
    """Test decoding using tiktoken."""
    hutoken.initialize('./vocabs/gpt2-vocab.txt')
    tt_enc = tiktoken.get_encoding("gpt2")

    word = "entropy"
    encoded = tt_enc.encode(word)
    print(f"Encoded tokens (tiktoken): {encoded}")
    decoded = hutoken.decode(encoded)
    print(f"Decoded text (tiktoken): {decoded}")

    assert decoded == word, f"Decoded text does not match original. Decoded: {decoded}, Original: {word}"


def test_decode_whole_sentence_using_tiktoken_encode():
    """Test decoding a whole sentence encoded using tiktoken."""
    hutoken.initialize('./vocabs/gpt2-vocab.txt')
    tt_enc = tiktoken.get_encoding("gpt2")

    sentence = "How can the net amount of entropy of the universe be massively decreased?"
    encoded = tt_enc.encode(sentence)
    print(f"Encoded tokens (tiktoken): {encoded}")
    decoded = hutoken.decode(encoded)
    print(f"Decoded text (tiktoken): {decoded}")

    assert decoded == sentence, f"Decoded text does not match original. Decoded: {decoded}, Original: {sentence}"

def test_huggingface_initialize_and_encode_decode():
    """Test hutoken Hugging Face integration with a model name."""
    import pytest  # Import pytest here to avoid UnboundLocalError
    
    try:
        import transformers
    except ImportError:
        pytest.skip("transformers not installed")

    # Try a model we know has a vocab dictionary
    model_name = "NYTK/PULI-LlumiX-32K"
    
    # Include Unicode, special characters, and multi-byte sequences
    test_text = "Ez egy Hugging Face teszt! üòä „Åì„Çì„Å´„Å°„ÅØ [CLS] <|endoftext|>"
    
    try:
        # Check if model has a vocab attribute before testing
        hf_tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
        if not hasattr(hf_tokenizer, "vocab"):
            pytest.skip(f"Model {model_name} doesn't have a .vocab attribute")
            
        # Initialize hutoken with a Hugging Face model
        hutoken.initialize(model_name)
        
        # Encode and decode using hutoken
        tokens = hutoken.encode(test_text)
        decoded = hutoken.decode(tokens)
        
        # Encode and decode using transformers directly
        hf_tokens = hf_tokenizer.encode(test_text, add_special_tokens=False)
        hf_decoded = hf_tokenizer.decode(hf_tokens)
        
        # Check decoded text rather than token IDs
        assert decoded == hf_decoded, f"Decoded text differs: {decoded} vs {hf_decoded}"
        
        # Optional: Compare token counts, which should be same even if IDs differ
        assert len(tokens) == len(hf_tokens), f"Token count differs: {len(tokens)} vs {len(hf_tokens)}"
        
    except Exception as e:
        pytest.fail(f"Test failed with error: {e}")
